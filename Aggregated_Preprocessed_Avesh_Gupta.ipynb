{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f75c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6bf277",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath= \"C:/Users/gupta/OneDrive/Desktop/New_folder/Internship_UBER/Dataset_Uber_Traffic.csv\"\n",
    "filepath2= 'c:/Users/gupta/OneDrive/Desktop/New_folder/Internship_UBER/Weather_Data.csv'\n",
    "filepath3= 'c:/Users/gupta/OneDrive/Desktop/New_folder/Internship_UBER/Calendar_Data.csv'\n",
    "df= pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f04a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da42a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in the dataframe\n",
    "null_values = df.isnull().sum()\n",
    "\n",
    "# Display the count of null values for each column\n",
    "print(null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2530e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df.duplicated()\n",
    "# Display rows that are duplicates\n",
    "df[duplicates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d905b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values by filling them with 0, though the data doesn't have any missing values nor any duplicates.\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Remove duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Correcting the data types, although the data type for the columns were correct except for DateTime column\n",
    "df = df.astype({\n",
    "    'Junction': 'int64',\n",
    "    'Vehicles': 'int64',\n",
    "    'ID': 'int64'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934d590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring the DateTime column is in datetime format\n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'], format='%d/%m/%y %H:%M', dayfirst=True)\n",
    "\n",
    "# Aggregating traffic data into hourly intervals for each junction\n",
    "hourly_traffic = df.groupby([df['DateTime'].dt.floor('H'), 'Junction']).agg({'Vehicles': 'sum'}).reset_index()\n",
    "\n",
    "# Renaming columns for clarity\n",
    "hourly_traffic.rename(columns={'DateTime': 'Hour', 'Vehicles': 'TotalVehicles'}, inplace=True)\n",
    "\n",
    "# Displaying the aggregated data\n",
    "hourly_traffic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63125262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalizing the TotalVehicles column\n",
    "hourly_traffic['NormalizedVehicles'] = scaler.fit_transform(hourly_traffic[['TotalVehicles']])\n",
    "\n",
    "# Displaying the updated dataframe\n",
    "hourly_traffic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb7ec4",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Generate time-based features\n",
    "df['HourOfDay'] = df['DateTime'].dt.hour\n",
    "df['DayOfWeek'] = df['DateTime'].dt.dayofweek\n",
    "df['Month'] = df['DateTime'].dt.month\n",
    "\n",
    "# Develop lag features (e.g., traffic data from the previous hour)\n",
    "df['Lag1HourVehicles'] = df['Vehicles'].shift(1)\n",
    "df['Lag24HourVehicles'] = df['Vehicles'].shift(24)\n",
    "\n",
    "# Update special event dates as per the Indian calendar\n",
    "special_event_dates = ['2015-08-15', '2015-10-22', '2015-11-11', '2016-08-15', '2016-10-30', '2016-11-07']  # Example dates for Independence Day, Dussehra, and Diwali\n",
    "special_event_dates = pd.to_datetime(special_event_dates)\n",
    "df['IsSpecialEvent'] = df['DateTime'].dt.date.isin(special_event_dates.date).astype(int)\n",
    "\n",
    "# Create the 'IsWeekend' column based on 'DayOfWeek'\n",
    "df['IsWeekend'] = df['DayOfWeek'].apply(lambda x: 1 if x in [5, 6] else 0)\n",
    "\n",
    "# Display the updated dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ad17f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis for numerical features\n",
    "correlation_matrix = df.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Prepare data for Random Forest feature importance\n",
    "\n",
    "X = df[['HourOfDay', 'DayOfWeek', 'Month', 'Lag1HourVehicles', 'Lag24HourVehicles', 'IsWeekend', 'IsSpecialEvent']]\n",
    "y = df['Vehicles']\n",
    "\n",
    "# Handle missing values in lag features\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Extract feature importance\n",
    "feature_importances = rf_model.feature_importances_\n",
    "features = X.columns\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=feature_importances, y=features, palette='viridis')\n",
    "plt.title(\"Feature Importance from Random Forest\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74e4b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions using the Random Forest model\n",
    "y_pred = rf_model.predict(X)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "r2 = r2_score(y, y_pred)\n",
    "mae = mean_absolute_error(y, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "# Display the metrics\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64126f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(filepath2, index_col='time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a8e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_data= pd.read_csv(filepath)\n",
    "weather_data= pd.read_csv(filepath2)\n",
    "events_data= pd.read_csv(filepath3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaaa1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DateTime columns to datetime format for merging\n",
    "traffic_data['DateTime'] = pd.to_datetime(traffic_data['DateTime'], format='%d/%m/%y %H:%M')\n",
    "weather_data['time'] = pd.to_datetime(weather_data['time'])\n",
    "events_data['DateTime'] = pd.to_datetime(events_data['DateTime'], format='%d-%m-%Y %H:%M')\n",
    "\n",
    "# Merge datasets on DateTime\n",
    "merged_data = pd.merge(traffic_data, weather_data, left_on='DateTime', right_on='time', how='inner')\n",
    "merged_data = pd.merge(merged_data, events_data, on='DateTime', how='inner')\n",
    "\n",
    "# Drop redundant 'time' column from weather_data\n",
    "merged_data.drop(columns=['time'], inplace=True)\n",
    "\n",
    "# Display the merged dataset\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0873a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.rename(columns={\n",
    "\t\"temperature_2m (°C)\": \"Temperature (°C)\",\n",
    "\t\"wind_speed_10m (km/h)\": \"Wind Speed (km/h)\",\n",
    "\t\"precipitation (mm)\": \"Precipitation (mm)\",\n",
    "\t\"relative_humidity_2m (%)\": \"Humidity (%)\"\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178cedb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'IsWeekend' column based on the 'DateTime' column\n",
    "merged_data['IsWeekend'] = merged_data['DateTime'].dt.dayofweek.apply(lambda x: 1 if x in [5, 6] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa22c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the updated DataFrame\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e68f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Assign distance based on Junction number\n",
    "# Each junction is 15 km apart\n",
    "junction_distance_map = {\n",
    "    1: 15,\n",
    "    2: 30,\n",
    "    3: 45,\n",
    "    4: 60\n",
    "}\n",
    "merged_data['Distance_km'] = merged_data['Junction'].map(junction_distance_map)\n",
    "\n",
    "# STEP 2: Define Fare Estimation Function\n",
    "def calculate_estimated_fare(row):\n",
    "    base_fare = 150  # base minimum fare\n",
    "    distance_charge = row['Distance_km'] * 10  # ₹10 per km\n",
    "    traffic_charge = row['Vehicles'] * 0.6     # moderate traffic impact\n",
    "    temp_effect = (row['Temperature (°C)'] - 20) * 0.4\n",
    "    rain_effect = row['Precipitation (mm)'] * 1.2\n",
    "    holiday_charge = 75 if row['Is Holiday'] == 1 else 0\n",
    "    weekend_charge = 50 if row['IsWeekend'] == 1 else 0\n",
    "\n",
    "    total_fare = base_fare + distance_charge + traffic_charge + temp_effect + rain_effect + holiday_charge + weekend_charge\n",
    "    return round(total_fare, 2)\n",
    "\n",
    "# STEP 3: Apply Fare Calculation\n",
    "merged_data['EstimatedFare'] = merged_data.apply(calculate_estimated_fare, axis=1)\n",
    "\n",
    "# STEP 4: Plot Fare vs Distance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=merged_data['Distance_km'], y=merged_data['EstimatedFare'], alpha=0.5)\n",
    "plt.title(\"Estimated Fare vs Distance (Junction-Based Distance)\")\n",
    "plt.xlabel(\"Distance (km)\")\n",
    "plt.ylabel(\"Estimated Fare\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# STEP 5: Check correlation (Optional)\n",
    "corr = merged_data[['Distance_km', 'EstimatedFare']].corr().iloc[0, 1]\n",
    "print(f\"Correlation between Distance and Estimated Fare: {corr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a4e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_fare = merged_data.groupby('Distance_km')['EstimatedFare'].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=avg_fare, x='Distance_km', y='EstimatedFare', marker='o')\n",
    "plt.title(\"Average Estimated Fare by Distance\")\n",
    "plt.xlabel(\"Distance (km)\")\n",
    "plt.ylabel(\"Avg Fare\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3f610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=merged_data, x='Vehicles', y='EstimatedFare', alpha=0.4)\n",
    "plt.title(\"Traffic Volume vs Estimated Fare\")\n",
    "plt.xlabel(\"Number of Vehicles (Traffic)\")\n",
    "plt.ylabel(\"Estimated Fare\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f5f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly = merged_data.groupby('HourOfDay')[['Vehicles', 'EstimatedFare']].mean().reset_index()\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "sns.lineplot(data=hourly, x='HourOfDay', y='Vehicles', ax=ax1, label='Avg Traffic', color='red')\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(data=hourly, x='HourOfDay', y='EstimatedFare', ax=ax2, label='Avg Fare', color='blue')\n",
    "ax1.set_xlabel(\"Hour of Day\")\n",
    "ax1.set_ylabel(\"Avg Traffic (Vehicles)\", color='red')\n",
    "ax2.set_ylabel(\"Avg Estimated Fare\", color='blue')\n",
    "plt.title(\"Hourly Traffic vs Fare Pattern\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb5b774",
   "metadata": {},
   "outputs": [],
   "source": [
    "junction_avg = merged_data.groupby('Junction')[['Vehicles', 'EstimatedFare']].mean().reset_index()\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "sns.barplot(data=junction_avg, x='Junction', y='Vehicles', color='orange', ax=ax1)\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(data=junction_avg, x='Junction', y='EstimatedFare', marker='o', color='blue', ax=ax2)\n",
    "ax1.set_xlabel(\"Junction\")\n",
    "ax1.set_ylabel(\"Avg Vehicles\", color='orange')\n",
    "ax2.set_ylabel(\"Avg Estimated Fare\", color='blue')\n",
    "plt.title(\"Junction-wise Avg Traffic and Fare\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ffde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5c1b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.to_csv('merged_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a866af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the model (RandomForestRegressor is already imported and initialized as rf_model)\n",
    "model = rf_model\n",
    "\n",
    "# Define evaluation metrics function\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, rmse, r2\n",
    "\n",
    "# Time-based cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "mae_scores, rmse_scores, r2_scores = [], [], []\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mae, rmse, r2 = evaluate_model(y_test, y_pred)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "# Display cross-validation results\n",
    "print(f\"Mean MAE: {np.mean(mae_scores):.4f}, Std: {np.std(mae_scores):.4f}\")\n",
    "print(f\"Mean RMSE: {np.mean(rmse_scores):.4f}, Std: {np.std(rmse_scores):.4f}\")\n",
    "print(f\"Mean R^2: {np.mean(r2_scores):.4f}, Std: {np.std(r2_scores):.4f}\")\n",
    "\n",
    "# Final model training on the entire dataset\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict on the entire dataset for final evaluation\n",
    "y_pred_final = model.predict(X)\n",
    "final_mae, final_rmse, final_r2 = evaluate_model(y, y_pred_final)\n",
    "\n",
    "# Display final evaluation metrics\n",
    "print(f\"Final Model Evaluation:\")\n",
    "print(f\"MAE: {final_mae:.4f}\")\n",
    "print(f\"RMSE: {final_rmse:.4f}\")\n",
    "print(f\"R^2: {final_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9bcd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the Gradient Boosting Regressor model\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Time-based cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Perform grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=tscv, scoring='neg_mean_squared_error', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "y_pred = best_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(f\"Best Model Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Validation MAE: {mae:.4f}\")\n",
    "print(f\"Validation RMSE: {rmse:.4f}\")\n",
    "print(f\"Validation R^2: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d453c70d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
